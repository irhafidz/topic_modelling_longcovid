{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Non-negative Matrix Factorization (NMF) Training",
   "id": "943c689325afd3cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dependencies",
   "id": "d32139764dec2e2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "from gensim import corpora, models\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from src.utils.topic_diversity import topic_diversity\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "5f7ea6d8505d19ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "228567258919fb25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Processed Data\n",
    "df = pd.read_csv(\"../data/processed/20250516_1955_clean_merged_tweets.csv\")\n",
    "df.info()"
   ],
   "id": "17b341d943636f22",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation / Config",
   "id": "57287195f247220b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CONFIGURATION FOR SAVING\n",
    "model_name = 'NMF'\n",
    "\n",
    "# Get today's date in YYYYMMDD format\n",
    "date_today = datetime.datetime.today().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Saved dir path\n",
    "results_dir = f\"../results/{date_today}_{model_name}\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Set Top-N of words\n",
    "TOP_DIVERSITY_WORDS_N = 30\n",
    "TOP_COHERENCE_WORDS_N = 10\n",
    "\n",
    "# Tokenize\n",
    "df['tokenized_content'] = df['final_text'].apply(lambda x: str(x).split())\n",
    "texts = df['tokenized_content'].tolist()\n",
    "docs_raw = df['final_text'].astype(str).tolist()"
   ],
   "id": "28b01242bf5920f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Filter Extremes (no_below, no_above, max_feat)",
   "id": "b229af94692f1f53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameter grid\n",
    "no_below_values = [2, 5, 10, 50, 100]\n",
    "no_above_values = [0.5, 0.7, 0.95]\n",
    "max_features_values = [30000, 50000, 70000, 90000]\n",
    "\n",
    "\n",
    "# Fixed model settings\n",
    "NUM_TOPICS = 16\n",
    "\n",
    "# Store results\n",
    "filter_extremes_hyperparameter = []\n",
    "best_coherence = -1\n",
    "best_filter_extremes = None\n",
    "\n",
    "for no_below, no_above, max_feat in itertools.product(no_below_values, no_above_values, max_features_values):\n",
    "    print(f\"\\n Trying min_df={no_below}, max_df={no_above}, max_features={max_feat}\")\n",
    "\n",
    "    # TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer(min_df=no_below, max_df=no_above, max_features=max_feat)\n",
    "    try:\n",
    "        X_tfidf = vectorizer.fit_transform(docs_raw)\n",
    "    except ValueError:\n",
    "        print(f\" Skipping: not enough terms with min_df={no_below}, max_df={no_above}\")\n",
    "        continue\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Skip if vocabulary is too small\n",
    "    if len(feature_names) < 10:\n",
    "        print(f\" Skipping min_df={no_below}, max_df={no_above} â€” vocab too small ({len(feature_names)} terms)\")\n",
    "        continue\n",
    "\n",
    "    # Train NMF Model\n",
    "    nmf_model = NMF(n_components=NUM_TOPICS)\n",
    "    W = nmf_model.fit_transform(X_tfidf)\n",
    "    H = nmf_model.components_\n",
    "\n",
    "    # Recreate gensim dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=max_feat)\n",
    "\n",
    "    # OPTIONAL: Match dictionary to vectorizer vocab (optional)\n",
    "    vocab_set = set(feature_names)\n",
    "    dictionary.filter_tokens(bad_ids=[tokenid for tokenid, token in dictionary.iteritems() if token not in vocab_set])\n",
    "    dictionary.compactify()\n",
    "\n",
    "    # Generate topic words\n",
    "    topics = []\n",
    "    for topic_idx in range(NUM_TOPICS):\n",
    "        top_words_idx = H[topic_idx].argsort()[::-1][:30]\n",
    "        topic_words = [feature_names[i] for i in top_words_idx]\n",
    "        topics.append(topic_words)\n",
    "\n",
    "    # Coherence evaluation\n",
    "    coherence_model = models.CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Store result\n",
    "    filter_extremes_hyperparameter.append({\n",
    "        \"min_df\": no_below,\n",
    "        \"max_df\": no_above,\n",
    "        \"max_features\": max_feat,\n",
    "        \"coherence\": coherence_score,\n",
    "        \"vocab_size\": len(feature_names),\n",
    "        \"num_topics\": NUM_TOPICS\n",
    "    })\n",
    "\n",
    "    print(f\" min_df={no_below}, max_df={no_above}, max_features={max_feat}, vocab={len(feature_names)}  coherence={coherence_score:.4f}\")\n",
    "\n",
    "    # Track best\n",
    "    if coherence_score > best_coherence:\n",
    "        best_coherence = coherence_score\n",
    "        best_filter_extremes = (no_below, no_above, max_feat)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest Filtering Parameters:\")\n",
    "print(f\"Best min_df: {best_filter_extremes[0]}, max_df: {best_filter_extremes[1]}, max_features: {best_filter_extremes[2]}, Coherence: {best_coherence:.4f}\")\n",
    "\n",
    "# Save Results\n",
    "df_filter_extremes_hyperparameter = pd.DataFrame(filter_extremes_hyperparameter)\n",
    "df_filter_extremes_hyperparameter.to_csv(os.path.join(results_dir, f\"nmf_filter_extremes_hyperparameter_{date_today}.csv\"), index=False)\n",
    "print(f\"Filter extremes results saved in: {results_dir}\")"
   ],
   "id": "1fb6b3ce0465bc2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Dict Based on Best Filter Params",
   "id": "59c3a024f6da589b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_MIN_DF = best_filter_extremes[0]\n",
    "BEST_MAX_DF = best_filter_extremes[1]\n",
    "BEST_MAX_FEATURES = best_filter_extremes[2]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=BEST_MIN_DF, no_above=BEST_MAX_DF, keep_n=BEST_MAX_FEATURES)\n",
    "\n",
    "# Prepare TF-IDF vectorizer with best filtering\n",
    "vectorizer = TfidfVectorizer(min_df=BEST_MIN_DF, max_df=BEST_MAX_DF, max_features=BEST_MAX_FEATURES)\n",
    "X_tfidf = vectorizer.fit_transform(docs_raw)\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ],
   "id": "45a45a0f6751181d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Num Topics (num_topics)",
   "id": "5a9f246662dba52a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define range for num_topics (n_components)\n",
    "num_topics_range = list(range(2, 27))\n",
    "\n",
    "# Store results\n",
    "topic_num_tuning_results = []\n",
    "best_coherence = -1\n",
    "best_num_topics = None\n",
    "\n",
    "# Grid search over number of topics (n_components)\n",
    "for num_topics in num_topics_range:\n",
    "    print(f\"\\n Trying num_topics (n_components) = {num_topics}\")\n",
    "\n",
    "    try:\n",
    "        # Train NMF model\n",
    "        nmf_model = NMF(\n",
    "            n_components=num_topics,\n",
    "        )\n",
    "        W = nmf_model.fit_transform(X_tfidf)\n",
    "        H = nmf_model.components_\n",
    "\n",
    "        # Extract top words per topic for coherence scoring\n",
    "        topics = []\n",
    "        for topic_idx in range(num_topics):\n",
    "            top_word_indices = H[topic_idx].argsort()[::-1][:30]\n",
    "            topic_words = [feature_names[i] for i in top_word_indices]\n",
    "            topics.append(topic_words)\n",
    "\n",
    "        # Compute coherence score using Gensim\n",
    "        coherence_model = models.CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "        # Store result\n",
    "        topic_num_tuning_results.append({\n",
    "            \"num_topics\": num_topics,\n",
    "            \"coherence\": coherence_score,\n",
    "            \"min_df\": BEST_MIN_DF,\n",
    "            \"max_df\": BEST_MAX_DF,\n",
    "            \"max_features\": BEST_MAX_FEATURES,\n",
    "            \"dictionary_size\": len(dictionary),\n",
    "        })\n",
    "\n",
    "        print(f\" num_topics={num_topics}  coherence={coherence_score:.4f}\")\n",
    "\n",
    "        if coherence_score > best_coherence:\n",
    "            best_coherence = coherence_score\n",
    "            best_num_topics = num_topics\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Skipping num_topics={num_topics} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest Number of Topics:\")\n",
    "print(f\"Best num_topics: {best_num_topics}, Best Coherence Score: {best_coherence:.4f}\")\n",
    "\n",
    "# Save results\n",
    "df_topic_num_tuning = pd.DataFrame(topic_num_tuning_results)\n",
    "df_topic_num_tuning.to_csv(os.path.join(results_dir, f\"nmf_num_topics_hyperparameter_{date_today}.csv\"), index=False)\n",
    "print(f\"Topic number tuning results saved in: {results_dir}\")"
   ],
   "id": "efd4daf9a1ea8fbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualize num_topics Grid Search",
   "id": "509cb9e8a46e36e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(df_topic_num_tuning[\"num_topics\"], df_topic_num_tuning[\"coherence\"], marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score (c_v)\")\n",
    "plt.title(\"NMF Topic Coherence vs Number of Topics\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "44fa14f235f446d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Best Params (alpha_W, alpha_H, l1_ratio)",
   "id": "e30e8a2e9a076fd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_NUM_TOPICS = best_num_topics\n",
    "\n",
    "#  Parameter grid\n",
    "alpha_Ws = [0.0, 0.01, 0.1, 0.5]\n",
    "alpha_Hs = [0.0, 0.01, 0.1, 0.5]\n",
    "l1_ratios = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "init_values = ['random', 'nndsvd', 'nndsvda', 'nndsvdar']\n",
    "\n",
    "#  Grid search\n",
    "regularization_tuning_results = []\n",
    "best_coherence = -1\n",
    "best_params = None\n",
    "\n",
    "for init_value, alpha_W, alpha_H, l1_ratio in itertools.product(init_values, alpha_Ws, alpha_Hs, l1_ratios):\n",
    "    print(f\"\\n Trying init={init_value}, alpha_W={alpha_W}, alpha_H={alpha_H}, l1_ratio={l1_ratio}\")\n",
    "\n",
    "    try:\n",
    "        # Fit NMF\n",
    "        nmf_model = NMF(\n",
    "            n_components=BEST_NUM_TOPICS,\n",
    "            init=init_value,\n",
    "            alpha_W=alpha_W,\n",
    "            alpha_H=alpha_H,\n",
    "            l1_ratio=l1_ratio,\n",
    "        )\n",
    "        W = nmf_model.fit_transform(X_tfidf)\n",
    "        H = nmf_model.components_\n",
    "\n",
    "        # Topic-word extraction\n",
    "        topics = []\n",
    "        for topic_idx in range(BEST_NUM_TOPICS):\n",
    "            top_word_indices = H[topic_idx].argsort()[::-1][:30]\n",
    "            topic_words = [feature_names[i] for i in top_word_indices]\n",
    "            topics.append(topic_words)\n",
    "\n",
    "        # Coherence Score\n",
    "        coherence_model = models.CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "        coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "        # Store result\n",
    "        regularization_tuning_results.append({\n",
    "            \"init_value\": init_value,\n",
    "            \"alpha_W\": alpha_W,\n",
    "            \"alpha_H\": alpha_H,\n",
    "            \"l1_ratio\": l1_ratio,\n",
    "            \"coherence\": coherence_score,\n",
    "            \"min_df\": BEST_MIN_DF,\n",
    "            \"max_df\": BEST_MAX_DF,\n",
    "            \"max_features\": BEST_MAX_FEATURES,\n",
    "            \"num_topics\": BEST_NUM_TOPICS,\n",
    "            \"dictionary_size\": len(dictionary)\n",
    "        })\n",
    "\n",
    "        print(f\" coherence={coherence_score:.4f}\")\n",
    "\n",
    "        # Track best\n",
    "        if coherence_score > best_coherence:\n",
    "            best_coherence = coherence_score\n",
    "            best_params = (init_value, alpha_W, alpha_H, l1_ratio)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Skipping init={init_value}, Skipping alpha_W={alpha_W}, Skipping alpha_H={alpha_H}, l1_ratio={l1_ratio} due to error: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "#  Summary\n",
    "print(\"\\nBest Regularization Combination:\")\n",
    "print(f\"Best init: {best_params[0]}, Best alpha_W: {best_params[1]}, alpha_H: {best_params[2]}, l1_ratio: {best_params[3]}, Coherence Score: {best_coherence:.4f}\")\n",
    "\n",
    "# Save results\n",
    "df_regularization_tuning = pd.DataFrame(regularization_tuning_results)\n",
    "df_regularization_tuning.to_csv(os.path.join(results_dir, f\"nmf_regularization_hyperparameter_{date_today}.csv\"), index=False)\n",
    "print(f\"Stage 3 tuning results saved in: {results_dir}\")"
   ],
   "id": "63bc2c11bd50f069",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final Model",
   "id": "b7b42ae2ad48bc5a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_NO_BELOW = best_filter_extremes[0]\n",
    "BEST_NO_ABOVE = best_filter_extremes[1]\n",
    "BEST_MAX_FEATURES = best_filter_extremes[2]\n",
    "BEST_NUM_TOPICS = best_num_topics\n",
    "BEST_INIT = best_params[0]\n",
    "BEST_ALPHA_W = best_params[1]\n",
    "BEST_ALPHA_H = best_params[2]\n",
    "BEST_L1_RATIO = best_params[3]\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", X_tfidf.shape)\n",
    "print(\"Vocabulary size:\", len(feature_names))\n",
    "\n",
    "#  Train final NMF model\n",
    "nmf_model = NMF(\n",
    "    n_components=BEST_NUM_TOPICS,\n",
    "    init=BEST_INIT,\n",
    "    alpha_W=BEST_ALPHA_W,\n",
    "    alpha_H=BEST_ALPHA_H,\n",
    "    l1_ratio=BEST_L1_RATIO,\n",
    ")\n",
    "W = nmf_model.fit_transform(X_tfidf)\n",
    "H = nmf_model.components_\n",
    "\n",
    "#  Compute coherence score\n",
    "topics = []\n",
    "for topic_idx in range(BEST_NUM_TOPICS):\n",
    "    top_word_indices = H[topic_idx].argsort()[::-1][:TOP_DIVERSITY_WORDS_N]\n",
    "    topic_words = [feature_names[i] for i in top_word_indices]\n",
    "    topics.append(topic_words)\n",
    "\n",
    "coherence_model = models.CoherenceModel(topics=topics, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Final model coherence (c_v): {coherence_score:.4f}\")\n",
    "\n",
    "# Save topic-word distributions\n",
    "topic_word_list = []\n",
    "for topic_idx, topic_words in enumerate(topics):\n",
    "    for word in topic_words:\n",
    "        weight = H[topic_idx][feature_names.tolist().index(word)]\n",
    "        topic_word_list.append({\n",
    "            \"topic\": topic_idx,\n",
    "            \"word\": word,\n",
    "            \"weight\": weight\n",
    "        })\n",
    "\n",
    "df_topics = pd.DataFrame(topic_word_list)\n",
    "df_topics.to_csv(os.path.join(results_dir, f\"nmf_topic_word_distributions_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save document-topic distributions\n",
    "doc_topics = []\n",
    "for i, topic_weights in enumerate(W):\n",
    "    row = {\"doc_id\": i}\n",
    "    row.update({f\"topic_{t}\": topic_weights[t] for t in range(BEST_NUM_TOPICS)})\n",
    "    doc_topics.append(row)\n",
    "\n",
    "df_doc_topics = pd.DataFrame(doc_topics)\n",
    "df_doc_topics.to_csv(os.path.join(results_dir, f\"nmf_document_topic_distributions_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save final model summary\n",
    "summary = {\n",
    "    \"no_below\": BEST_NO_BELOW,\n",
    "    \"no_above\": BEST_NO_ABOVE,\n",
    "    \"max_features\": BEST_MAX_FEATURES,\n",
    "    \"num_topics\": BEST_NUM_TOPICS,\n",
    "    \"init\": BEST_INIT,\n",
    "    \"alpha_W\": BEST_ALPHA_W,\n",
    "    \"alpha_H\": BEST_ALPHA_H,\n",
    "    \"l1_ratio\": BEST_L1_RATIO,\n",
    "    \"coherence_score\": coherence_score,\n",
    "    \"dictionary_size\": len(dictionary),\n",
    "    \"num_documents\": len(docs_raw)\n",
    "}\n",
    "\n",
    "# Topic diversity\n",
    "top_n_values = [5, 10, 20, 30]\n",
    "diversity_score_results = []\n",
    "\n",
    "for top_n in top_n_values:\n",
    "    diversity_score = topic_diversity(nmf_model, top_n=top_n, model_type='nmf', feature_names=feature_names)\n",
    "    diversity_score_results.append({\"top_n\": top_n, \"topic_diversity\": diversity_score})\n",
    "    print(f\"top_n: {top_n} topic_diversity: {diversity_score:.4f}\")\n",
    "\n",
    "df_diversity = pd.DataFrame(diversity_score_results)\n",
    "df_diversity.to_csv(os.path.join(results_dir, f\"topic_diversity_scores_{date_today}.csv\"), index=False)\n",
    "\n",
    "for row in diversity_score_results:\n",
    "    summary[f\"diversity_score_top{row['top_n']}\"] = row[\"topic_diversity\"]\n",
    "\n",
    "df_summary = pd.DataFrame([summary])\n",
    "df_summary.to_csv(os.path.join(results_dir, f\"nmf_model_summary_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save Model\n",
    "joblib.dump(vectorizer, os.path.join(results_dir, f\"nmf_vectorizer_{date_today}.pkl\"))\n",
    "joblib.dump(nmf_model, os.path.join(results_dir, f\"nmf_model_{date_today}.pkl\"))\n",
    "\n",
    "print(f\" Final NMF model, topics, and distributions saved to: {results_dir}\")"
   ],
   "id": "24af2ec113668ab1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### View Top-30 Words per Topic",
   "id": "db1e8e57e06db5ac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic_word_data = []\n",
    "\n",
    "for topic_idx, topic_weights in enumerate(H):\n",
    "    top_indices = topic_weights.argsort()[::-1][:TOP_DIVERSITY_WORDS_N]\n",
    "    for rank, index in enumerate(top_indices, start=1):\n",
    "        topic_word_data.append({\n",
    "            \"topic\": topic_idx,\n",
    "            \"word_rank\": rank,\n",
    "            \"word\": feature_names[index],\n",
    "            \"weight\": topic_weights[index]\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_topic_words = pd.DataFrame(topic_word_data)\n",
    "\n",
    "# Save to CSV\n",
    "topic_words_filename = os.path.join(results_dir, f\"nmf_top{TOP_DIVERSITY_WORDS_N}_words_per_topic_{date_today}.csv\")\n",
    "df_topic_words.to_csv(topic_words_filename, index=False)\n",
    "\n",
    "print(f\"Top {TOP_DIVERSITY_WORDS_N} words per topic saved to: {topic_words_filename}\")\n",
    "\n",
    "# Sample 30 Words per Topic\n",
    "print(f\"\\n Top {TOP_DIVERSITY_WORDS_N} Words per Topic:\")\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "nmf_topics = []\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-TOP_DIVERSITY_WORDS_N - 1:-1]]\n",
    "    nmf_topics.append(f\"Topic {topic_idx + 1}: {', '.join(top_words)}\")\n",
    "\n",
    "for topic in nmf_topics:\n",
    "    print(topic)"
   ],
   "id": "65ce4e483164cb82",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
