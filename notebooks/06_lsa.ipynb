{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Latent Semantic Analysis (LSA) Training",
   "id": "661a0bcaa1614263"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dependencies",
   "id": "84f94c0a00103ec7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import datetime\n",
    "from gensim import corpora, models\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.topic_diversity import topic_diversity\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "a58a05eb3fb144b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "40d1ba7b2cb03239"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load Processed Data\n",
    "df = pd.read_csv(\"../data/processed/20250516_1955_clean_merged_tweets.csv\")\n",
    "df.info()"
   ],
   "id": "8157fba82c6e42d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation / Config",
   "id": "2f6ac619d8df48c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CONFIGURATION FOR SAVING\n",
    "model_name = 'LSA'\n",
    "\n",
    "# Get today's date in YYYYMMDD format\n",
    "date_today = datetime.datetime.today().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Saved dir path\n",
    "results_dir = f\"../results/{date_today}_{model_name}\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Set Top-N of words\n",
    "TOP_DIVERSITY_WORDS_N = 30\n",
    "TOP_COHERENCE_WORDS_N = 10\n",
    "\n",
    "# Tokenize\n",
    "df['tokenized_content'] = df['final_text'].apply(lambda x: str(x).split())\n",
    "texts = df['tokenized_content'].tolist()"
   ],
   "id": "4fd2f69e06b6f54b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Filter Extremes (no_below, no_above, keep_n)",
   "id": "d38c9c450faae048"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameter grid\n",
    "no_below_values = [2, 5, 10, 50, 100]\n",
    "no_above_values = [0.5, 0.7, 0.9, 0.95]\n",
    "keep_n_values = [30000, 50000, 70000, 90000]\n",
    "\n",
    "# Fixed model settings\n",
    "NUM_TOPICS = 16\n",
    "\n",
    "# Store results\n",
    "filter_extremes_hyperparameter = []\n",
    "best_coherence = -1\n",
    "best_filter_extremes = None\n",
    "\n",
    "for no_below, no_above, keep_n in itertools.product(no_below_values, no_above_values, keep_n_values):\n",
    "    # Build dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    dictionary.filter_extremes(no_below=no_below, no_above=no_above, keep_n=keep_n)\n",
    "\n",
    "    # Create corpus (BOW)\n",
    "    corpus_bow = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "    # Create TF-IDF model\n",
    "    tfidf_model = models.TfidfModel(corpus_bow)\n",
    "    corpus_tfidf = tfidf_model[corpus_bow]\n",
    "\n",
    "    # Train LSI model on TF-IDF corpus\n",
    "    lsi_model = models.LsiModel(\n",
    "        corpus=corpus_tfidf,\n",
    "        id2word=dictionary,\n",
    "        num_topics=NUM_TOPICS\n",
    "    )\n",
    "\n",
    "    # Compute coherence\n",
    "    coherence_model = models.CoherenceModel(model=lsi_model, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Store result\n",
    "    filter_extremes_hyperparameter.append({\n",
    "        \"no_below\": no_below,\n",
    "        \"no_above\": no_above,\n",
    "        \"keep_n\": keep_n,\n",
    "        \"coherence\": coherence_score,\n",
    "        \"dictionary_size\": len(dictionary),\n",
    "        \"num_topics\": NUM_TOPICS\n",
    "    })\n",
    "\n",
    "    print(f\"no_below={no_below}, no_above={no_above}, keep_n={keep_n}, dict_size={len(dictionary)} coherence={coherence_score:.4f}\")\n",
    "\n",
    "    # Track best\n",
    "    if coherence_score > best_coherence:\n",
    "        best_coherence = coherence_score\n",
    "        best_filter_extremes = (no_below, no_above, keep_n)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest Filtering Parameters:\")\n",
    "print(f\"Best no_below: {best_filter_extremes[0]}, Best no_above: {best_filter_extremes[1]}, Best keep_n: {best_filter_extremes[2]}, Best Coherence Score: {best_coherence:.4f}\")\n",
    "\n",
    "# Save Results\n",
    "df_filter_extremes_hyperparameter = pd.DataFrame(filter_extremes_hyperparameter)\n",
    "df_filter_extremes_hyperparameter.to_csv(os.path.join(results_dir, f\"filter_extremes_hyperparameter_lsa_{date_today}.csv\"), index=False)\n",
    "print(f\"Filter extremes results saved in: {results_dir}\")"
   ],
   "id": "1107e72092e81629",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create Dict Based on Best Filter Params",
   "id": "531b9134d4ce0662"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load or define the best parameters\n",
    "BEST_NO_BELOW = best_filter_extremes[0]\n",
    "BEST_NO_ABOVE = best_filter_extremes[1]\n",
    "BEST_KEEP_N = best_filter_extremes[2]\n",
    "\n",
    "# Create dictionary and apply best filtering\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.filter_extremes(no_below=BEST_NO_BELOW, no_above=BEST_NO_ABOVE, keep_n=BEST_KEEP_N)\n",
    "\n",
    "# Create BOW corpus\n",
    "corpus_bow = [dictionary.doc2bow(doc) for doc in texts]\n",
    "\n",
    "# Convert to TF-IDF\n",
    "tfidf_model = models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = tfidf_model[corpus_bow]"
   ],
   "id": "a0fe707cf518a129",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Num Topics (num_topics)",
   "id": "f475b5f4c46c0643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define num_topics range\n",
    "num_topics_range = list(range(2, 27))\n",
    "\n",
    "# Store results\n",
    "topic_num_tuning_results = []\n",
    "best_coherence = -1\n",
    "best_num_topics = None\n",
    "\n",
    "# Grid search over num_topics\n",
    "for num_topics in num_topics_range:\n",
    "    # Train LSA model\n",
    "    lsi_model = models.LsiModel(\n",
    "        corpus=corpus_tfidf,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics\n",
    "    )\n",
    "\n",
    "    # Compute coherence\n",
    "    coherence_model = models.CoherenceModel(model=lsi_model, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Store results\n",
    "    topic_num_tuning_results.append({\n",
    "        \"num_topics\": num_topics,\n",
    "        \"coherence\": coherence_score,\n",
    "        \"no_below\": BEST_NO_BELOW,\n",
    "        \"no_above\": BEST_NO_ABOVE,\n",
    "        \"keep_n\": BEST_KEEP_N,\n",
    "        \"dictionary_size\": len(dictionary)\n",
    "    })\n",
    "\n",
    "    print(f\"num_topics={num_topics} coherence={coherence_score:.4f}\")\n",
    "\n",
    "    if coherence_score > best_coherence:\n",
    "        best_coherence = coherence_score\n",
    "        best_num_topics = num_topics\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest Number of Topics:\")\n",
    "print(f\"Best num_topics: {best_num_topics}, Best Coherence Score: {best_coherence:.4f}\")\n",
    "\n",
    "# Save results\n",
    "df_topic_num_tuning = pd.DataFrame(topic_num_tuning_results)\n",
    "df_topic_num_tuning.to_csv(os.path.join(results_dir, f\"num_topics_hyperparameter_lsa_{date_today}.csv\"), index=False)\n",
    "print(f\"LSA num_topics tuning results saved in: {results_dir}\")"
   ],
   "id": "35d606ba0a885a2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualize num_topics Grid Search",
   "id": "cfa4fbed9850a42c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.plot(df_topic_num_tuning[\"num_topics\"], df_topic_num_tuning[\"coherence\"], marker='o')\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score (c_v)\")\n",
    "plt.title(\"LSA Topic Coherence vs Number of Topics\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "7d91ac9c9010ad59",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Best Params (chunk_size, decay, power_iters)",
   "id": "7ee33399526c380f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_NUM_TOPICS = best_num_topics\n",
    "\n",
    "# Grid Parameters\n",
    "chunksize_values = [20000, 50000, 70000, 100000]\n",
    "decay_values = [0.3, 0.5, 1.0]\n",
    "power_iters_values = [2, 5, 10]\n",
    "\n",
    "# Store results\n",
    "lsa_param_tuning_results = []\n",
    "best_coherence = -1\n",
    "best_lsa_params = None\n",
    "\n",
    "# Grid search over chunksize, decay, power_iters\n",
    "for chunksize, decay, power_iters in itertools.product(chunksize_values, decay_values, power_iters_values):\n",
    "    # Train LSA model with custom parameters\n",
    "    lsi_model = models.LsiModel(\n",
    "        corpus=corpus_tfidf,\n",
    "        id2word=dictionary,\n",
    "        num_topics=BEST_NUM_TOPICS,\n",
    "        chunksize=chunksize,\n",
    "        decay=decay,\n",
    "        power_iters=power_iters\n",
    "    )\n",
    "\n",
    "    # Coherence score\n",
    "    coherence_model = models.CoherenceModel(model=lsi_model, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Store results\n",
    "    lsa_param_tuning_results.append({\n",
    "        \"chunksize\": chunksize,\n",
    "        \"decay\": decay,\n",
    "        \"power_iters\": power_iters,\n",
    "        \"coherence\": coherence_score,\n",
    "        \"no_below\": BEST_NO_BELOW,\n",
    "        \"no_above\": BEST_NO_ABOVE,\n",
    "        \"keep_n\": BEST_KEEP_N,\n",
    "        \"num_topics\": BEST_NUM_TOPICS,\n",
    "        \"dictionary_size\": len(dictionary)\n",
    "    })\n",
    "\n",
    "    print(f\"chunksize={chunksize}, decay={decay}, power_iters={power_iters} coherence={coherence_score:.4f}\")\n",
    "\n",
    "    if coherence_score > best_coherence:\n",
    "        best_coherence = coherence_score\n",
    "        best_lsa_params = (chunksize, decay, power_iters)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest LSA Parameters:\")\n",
    "print(f\"Best chunksize: {best_lsa_params[0]}, Best decay: {best_lsa_params[1]}, Best power_iters: {best_lsa_params[2]}, Best Coherence Score: {best_coherence:.4f}\")\n",
    "\n",
    "# Save results\n",
    "df_lsa_param_tuning = pd.DataFrame(lsa_param_tuning_results)\n",
    "df_lsa_param_tuning.to_csv(os.path.join(results_dir, f\"lsa_param_tuning_{date_today}.csv\"), index=False)\n",
    "print(f\"LSA parameter tuning results saved in: {results_dir}\")"
   ],
   "id": "15ed941ccd67d00a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final Model",
   "id": "57621fa6496dcc57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_NO_BELOW = best_filter_extremes[0]\n",
    "BEST_NO_ABOVE = best_filter_extremes[1]\n",
    "BEST_KEEP_N = best_filter_extremes[2]\n",
    "BEST_NUM_TOPICS = best_num_topics\n",
    "BEST_CHUNKSIZE = best_lsa_params[0]\n",
    "BEST_DECAY = best_lsa_params[1]\n",
    "BEST_POWER_ITERS = best_lsa_params[2]\n",
    "\n",
    "# Train final LSA model\n",
    "lsi_model = models.LsiModel(\n",
    "    corpus=corpus_tfidf,\n",
    "    id2word=dictionary,\n",
    "    num_topics=BEST_NUM_TOPICS,\n",
    "    chunksize=BEST_CHUNKSIZE,\n",
    "    decay=BEST_DECAY,\n",
    "    power_iters=BEST_POWER_ITERS\n",
    ")\n",
    "\n",
    "# Compute coherence score\n",
    "coherence_model = models.CoherenceModel(model=lsi_model, texts=texts, dictionary=dictionary, coherence='c_v', topn=TOP_COHERENCE_WORDS_N)\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"🎯 Final model coherence (c_v): {coherence_score:.4f}\")\n",
    "\n",
    "# Save model and artifacts\n",
    "lsi_model.save(os.path.join(results_dir, f\"lsa_model_{date_today}.gensim\"))\n",
    "dictionary.save(os.path.join(results_dir, f\"lsa_dictionary_{date_today}.dict\"))\n",
    "corpora.MmCorpus.serialize(os.path.join(results_dir, f\"lsa_corpus_{date_today}.mm\"), corpus_bow)\n",
    "\n",
    "# Save topic-word distributions to CSV\n",
    "topics = lsi_model.show_topics(num_topics=BEST_NUM_TOPICS, num_words=TOP_DIVERSITY_WORDS_N, formatted=False)\n",
    "topic_word_list = []\n",
    "for topic_num, topic_words in topics:\n",
    "    for word, weight in topic_words:\n",
    "        topic_word_list.append({\n",
    "            \"topic\": topic_num,\n",
    "            \"word\": word,\n",
    "            \"weight\": weight\n",
    "        })\n",
    "\n",
    "df_topics = pd.DataFrame(topic_word_list)\n",
    "df_topics.to_csv(os.path.join(results_dir, f\"lsa_topic_word_distributions_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save document-topic distributions\n",
    "doc_topics = []\n",
    "for i, doc_bow in enumerate(corpus_bow):\n",
    "    topic_dist = lsi_model[doc_bow]\n",
    "    row = {\"doc_id\": i}\n",
    "    row.update({f\"topic_{t[0]}\": t[1] for t in topic_dist})\n",
    "    doc_topics.append(row)\n",
    "\n",
    "df_doc_topics = pd.DataFrame(doc_topics)\n",
    "df_doc_topics.to_csv(os.path.join(results_dir, f\"lsa_document_topic_distributions_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save final summary\n",
    "summary = {\n",
    "    \"no_below\": BEST_NO_BELOW,\n",
    "    \"no_above\": BEST_NO_ABOVE,\n",
    "    \"num_topics\": BEST_NUM_TOPICS,\n",
    "    \"chunksize\": BEST_CHUNKSIZE,\n",
    "    \"decay\": BEST_DECAY,\n",
    "    \"power_iters\": BEST_POWER_ITERS,\n",
    "    \"keep_n\": BEST_KEEP_N,\n",
    "    \"coherence_score\": coherence_score,\n",
    "    \"dictionary_size\": len(dictionary),\n",
    "    \"num_documents\": len(corpus_bow),\n",
    "}\n",
    "\n",
    "# Diversity Scores\n",
    "top_n_values = [5, 10, 20, 30]\n",
    "diversity_score_results = []\n",
    "for top_n in top_n_values:\n",
    "    diversity_score = topic_diversity(lsi_model, top_n=top_n, model_type='lsa')\n",
    "    diversity_score_results.append({\"top_n\": top_n, \"topic_diversity\": diversity_score})\n",
    "    print(f\"top_n: {top_n} topic_diversity: {diversity_score}\")\n",
    "\n",
    "df_diversity = pd.DataFrame(diversity_score_results)\n",
    "df_diversity.to_csv(os.path.join(results_dir, f\"topic_diversity_scores_{date_today}.csv\"), index=False)\n",
    "\n",
    "for row in diversity_score_results:\n",
    "    summary[f\"diversity_score_top{row['top_n']}\"] = row[\"topic_diversity\"]\n",
    "\n",
    "# Save\n",
    "pd.DataFrame([summary]).to_csv(os.path.join(results_dir, f\"lsa_model_summary_{date_today}.csv\"), index=False)\n",
    "print(f\"Final LSA model, topics, and summaries saved to: {results_dir}\")"
   ],
   "id": "3a2b348c03175154",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### View Top-30 Words per Topic",
   "id": "dbd8c5545fcb50dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topics = lsi_model.show_topics(num_topics=BEST_NUM_TOPICS, num_words=TOP_DIVERSITY_WORDS_N, formatted=False)\n",
    "\n",
    "# Build structured list\n",
    "topic_word_data = []\n",
    "for topic_num, word_list in topics:\n",
    "    for rank, (word, weight) in enumerate(word_list, start=1):\n",
    "        topic_word_data.append({\n",
    "            \"topic\": topic_num,\n",
    "            \"word_rank\": rank,\n",
    "            \"word\": word,\n",
    "            \"weight\": weight\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_topic_words = pd.DataFrame(topic_word_data)\n",
    "\n",
    "# Save to CSV\n",
    "topic_words_filename = os.path.join(results_dir, f\"lsa_top{TOP_DIVERSITY_WORDS_N}_words_per_topic_{date_today}.csv\")\n",
    "df_topic_words.to_csv(topic_words_filename, index=False)\n",
    "\n",
    "print(f\"Top {TOP_DIVERSITY_WORDS_N} words per topic saved to: {topic_words_filename}\")\n",
    "\n",
    "# Sample 30 Words per Topic\n",
    "print(f\"\\nTop {TOP_DIVERSITY_WORDS_N} Words per Topic:\")\n",
    "for topic_num, word_list in topics:\n",
    "    words_only = [word for word, weight in word_list]\n",
    "    print(f\"Topic {topic_num}: {', '.join(words_only)}\")"
   ],
   "id": "63b9562b040dcaf3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
