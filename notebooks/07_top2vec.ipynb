{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Top2Vec Training",
   "id": "7e908fcf69f2bb2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dependencies",
   "id": "2989606779bd8221"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "from top2vec import Top2Vec\n",
    "from gensim import corpora, models\n",
    "from src.utils.topic_diversity import topic_diversity\n",
    "import itertools\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.CRITICAL)\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "13382b07479346c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Dataset",
   "id": "84ecff720bb4bc1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(\"../data/processed/20250515_1207_minimal_clean_merged_tweets.csv\")\n",
    "df.info()"
   ],
   "id": "ae865cc9052e56ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preparation / Config",
   "id": "e44b9004534f95aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# CONFIGURATION FOR SAVING\n",
    "model_name = 'TOP2VEC'\n",
    "\n",
    "# Get today's date in YYYYMMDD format\n",
    "date_today = datetime.datetime.today().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "# Saved dir path\n",
    "results_dir = f\"../results/{date_today}_{model_name}\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "TOP_DIVERSITY_WORDS_N = 30\n",
    "TOP_COHERENCE_WORDS_N = 10\n",
    "\n",
    "documents = df['final_text'].astype(str).tolist()\n",
    "print(f\"{len(documents)} rows from full dataset.\")\n",
    "\n",
    "tokenized_texts = [str(doc).split() for doc in documents]\n",
    "dictionary = corpora.Dictionary(tokenized_texts)"
   ],
   "id": "649799616f30c8b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter Embedding Models",
   "id": "561d2d29c417de17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define Embedding Params\n",
    "embedding_models = ['doc2vec', 'universal-sentence-encoder', 'universal-sentence-encoder-large', 'all-MiniLM-L6-v2', 'paraphrase-multilingual-MiniLM-L12-v2']\n",
    "\n",
    "# Store results\n",
    "embedding_model_hyperparameter = []\n",
    "best_coherence = -1\n",
    "best_embedding_model = None\n",
    "\n",
    "# Run grid search for embedding hyperparameter\n",
    "for embedding_model in embedding_models:\n",
    "    # Train Top2Vec model\n",
    "    top2vec_model = Top2Vec(\n",
    "        documents=documents,\n",
    "        embedding_model=embedding_model,\n",
    "        workers=10,\n",
    "    )\n",
    "\n",
    "    # Topic words\n",
    "    topics_words, _, _ = top2vec_model.get_topics()\n",
    "    topic_words_list = [topic[:TOP_DIVERSITY_WORDS_N] for topic in topics_words]\n",
    "\n",
    "    # Compute Coherence\n",
    "    coherence_model = models.CoherenceModel(\n",
    "        topics=topic_words_list,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v',\n",
    "        topn=TOP_COHERENCE_WORDS_N,\n",
    "    )\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Topic stats\n",
    "    topic_sizes, _ = top2vec_model.get_topic_sizes()\n",
    "    avg_topic_size = sum(topic_sizes) / len(topic_sizes)\n",
    "    num_topics = len(topics_words)\n",
    "\n",
    "    # Store result\n",
    "    embedding_model_hyperparameter.append({\n",
    "        \"embedding_model\": embedding_model,\n",
    "        \"num_topics\": num_topics,\n",
    "        \"avg_topic_size\": avg_topic_size,\n",
    "        \"coherence_c_v\": coherence_score,\n",
    "    })\n",
    "\n",
    "    print(f\"embedding_model={embedding_model}, num_topics={num_topics}, avg_topic_size={avg_topic_size} coherence={coherence_score:.4f}\")\n",
    "\n",
    "    # Track best\n",
    "    if coherence_score > best_coherence:\n",
    "        best_coherence = coherence_score\n",
    "        best_embedding_model = embedding_model\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest Filtering Parameters:\")\n",
    "print(f\"Best Embedding Model: {best_embedding_model}\")\n",
    "\n",
    "# Save Results\n",
    "df_embedding_model_hyperparameter = pd.DataFrame(embedding_model_hyperparameter)\n",
    "df_embedding_model_hyperparameter.to_csv(os.path.join(results_dir, f\"embedding_model_hyperparameter_{date_today}.csv\"), index=False)\n",
    "print(f\"Filter Embedding Model saved in: {results_dir} \")"
   ],
   "id": "631e35e25125c824",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter UMAP & HDBSCAN",
   "id": "bba05ec589f498d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_EMBEDDING = best_embedding_model\n",
    "\n",
    "# Define search ranges\n",
    "umap_args = [\n",
    "        {\"n_neighbors\": 15, \"min_dist\": 0.1, \"n_components\": 5, \"metric\": \"cosine\"},\n",
    "        {\"n_neighbors\": 30, \"min_dist\": 0.0, \"n_components\": 10, \"metric\": \"cosine\"},\n",
    "        {\"n_neighbors\": 10, \"min_dist\": 0.25, \"n_components\": 5, \"metric\": \"cosine\"}\n",
    "    ]\n",
    "\n",
    "hdbscan_args = [\n",
    "        {\"min_cluster_size\": 30, \"min_samples\": 10, \"cluster_selection_method\": \"eom\"},\n",
    "        {\"min_cluster_size\": 15, \"min_samples\": 5, \"cluster_selection_method\": \"eom\"},\n",
    "        {\"min_cluster_size\": 50, \"min_samples\": 15, \"cluster_selection_method\": \"leaf\"}\n",
    "    ]\n",
    "\n",
    "# Store results\n",
    "umap_hdbscan_hyperparameter = []\n",
    "best_coherence = -1\n",
    "best_umap_hdbscan = None\n",
    "\n",
    "# Run grid search for embedding hyperparameter\n",
    "for umap_arg, hdbscan_arg in itertools.product(umap_args, hdbscan_args):\n",
    "    # Train Top2Vec model\n",
    "    top2vec_model = Top2Vec(\n",
    "        documents=documents,\n",
    "        embedding_model=BEST_EMBEDDING,\n",
    "        workers=12,\n",
    "        umap_args=umap_arg,\n",
    "        hdbscan_args=hdbscan_arg,\n",
    "    )\n",
    "\n",
    "    # Topic words\n",
    "    topics_words, _, _ = top2vec_model.get_topics()\n",
    "    topic_words_list = [topic[:TOP_DIVERSITY_WORDS_N] for topic in topics_words]\n",
    "\n",
    "    # Compute Coherence\n",
    "    coherence_model = models.CoherenceModel(\n",
    "        topics=topic_words_list,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v',\n",
    "        topn=TOP_COHERENCE_WORDS_N,\n",
    "    )\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Topic stats\n",
    "    topic_sizes, _ = top2vec_model.get_topic_sizes()\n",
    "    avg_topic_size = sum(topic_sizes) / len(topic_sizes)\n",
    "    num_topics = len(topics_words)\n",
    "\n",
    "    # Store result\n",
    "    umap_hdbscan_hyperparameter.append({\n",
    "        \"embedding_model\": BEST_EMBEDDING,\n",
    "        \"umap_arg\": umap_arg,\n",
    "        \"hdbscan_arg\": hdbscan_arg,\n",
    "        \"num_topics\": num_topics,\n",
    "        \"avg_topic_size\": avg_topic_size,\n",
    "        \"coherence_c_v\": coherence_score,\n",
    "    })\n",
    "\n",
    "    print(f\"umap_arg={umap_arg}, hdbscan_arg={hdbscan_arg}, num_topics={num_topics}, avg_topic_size={avg_topic_size} coherence={coherence_score:.4f}\")\n",
    "\n",
    "    # Track best\n",
    "    if coherence_score > best_coherence:\n",
    "        best_coherence = coherence_score\n",
    "        best_umap_hdbscan = (umap_arg, hdbscan_arg)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest Umap HDBScan Parameters:\")\n",
    "print(f\"Best Umap Args: {best_umap_hdbscan[0]}, Best Hdbscan Args: {best_umap_hdbscan[1]}\")\n",
    "\n",
    "# Save Results\n",
    "df_umap_hdbscan_hyperparameter = pd.DataFrame(umap_hdbscan_hyperparameter)\n",
    "df_umap_hdbscan_hyperparameter.to_csv(os.path.join(results_dir, f\"umap_hdbscan_hyperparameter_{date_today}.csv\"), index=False)\n",
    "print(f\"Filter Umap HDBScan results saved in: {results_dir} \")"
   ],
   "id": "af90005f53dda6b4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hyperparameter min_counts & topic_merge_deltas",
   "id": "300b927bf4c7d32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_EMBEDDING = best_embedding_model\n",
    "BEST_UMAP_ARGS = best_umap_hdbscan[0]\n",
    "BEST_HDBSCAN_ARGS = best_umap_hdbscan[1]\n",
    "\n",
    "# Define search ranges\n",
    "min_counts = [30, 50, 100]\n",
    "topic_merge_deltas = [0.05, 0.1, 0.2]\n",
    "\n",
    "# Store results\n",
    "count_delta_hyperparameter = []\n",
    "best_coherence = -1\n",
    "best_count_delta = None\n",
    "\n",
    "# Run grid search for embedding hyperparameter\n",
    "for min_count, topic_merge_delta in itertools.product(min_counts, topic_merge_deltas):\n",
    "    # Train Top2Vec model\n",
    "    top2vec_model = Top2Vec(\n",
    "        documents=documents,\n",
    "        embedding_model=BEST_EMBEDDING,\n",
    "        workers=12,\n",
    "        umap_args=BEST_UMAP_ARGS,\n",
    "        hdbscan_args=BEST_HDBSCAN_ARGS,\n",
    "        min_count=min_count,\n",
    "        topic_merge_delta=topic_merge_delta,\n",
    "    )\n",
    "\n",
    "    # Topic words\n",
    "    topics_words, _, _ = top2vec_model.get_topics()\n",
    "    topic_words_list = [topic[:TOP_DIVERSITY_WORDS_N] for topic in topics_words]\n",
    "\n",
    "    # Compute Coherence\n",
    "    coherence_model = models.CoherenceModel(\n",
    "        topics=topic_words_list,\n",
    "        texts=tokenized_texts,\n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v',\n",
    "        topn=TOP_COHERENCE_WORDS_N,\n",
    "    )\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "\n",
    "    # Topic stats\n",
    "    topic_sizes, _ = top2vec_model.get_topic_sizes()\n",
    "    avg_topic_size = sum(topic_sizes) / len(topic_sizes)\n",
    "    num_topics = len(topics_words)\n",
    "\n",
    "    # Store result\n",
    "    count_delta_hyperparameter.append({\n",
    "        \"embedding_model\": BEST_EMBEDDING,\n",
    "        \"umap_arg\": BEST_UMAP_ARGS,\n",
    "        \"hdbscan_arg\": BEST_HDBSCAN_ARGS,\n",
    "        \"min_count\": min_count,\n",
    "        \"topic_merge_delta\": topic_merge_delta,\n",
    "        \"num_topics\": num_topics,\n",
    "        \"avg_topic_size\": avg_topic_size,\n",
    "        \"coherence_c_v\": coherence_score,\n",
    "    })\n",
    "\n",
    "    print(f\"min_count={min_count}, topic_merge_delta={topic_merge_delta}, num_topics={num_topics}, avg_topic_size={avg_topic_size} coherence={coherence_score:.4f}\")\n",
    "\n",
    "    # Track best\n",
    "    if coherence_score > best_coherence:\n",
    "        best_coherence = coherence_score\n",
    "        best_count_delta = (min_count, topic_merge_delta)\n",
    "\n",
    "\n",
    "# Summary\n",
    "print(\"\\nBest Count Delta:\")\n",
    "print(f\"Best Min Count: {best_count_delta[0]}, Best Merge Delta: {best_count_delta[1]}\")\n",
    "\n",
    "# Save Results\n",
    "df_count_delta_hyperparameter = pd.DataFrame(count_delta_hyperparameter)\n",
    "df_count_delta_hyperparameter.to_csv(os.path.join(results_dir, f\"count_delta_hyperparameter_{date_today}.csv\"), index=False)\n",
    "print(f\"Filter Count Delta results saved in: {results_dir} \")"
   ],
   "id": "b3cbc2135e2925",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Final Model",
   "id": "f714cee66ff265fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "BEST_EMBEDDING = best_embedding_model\n",
    "BEST_UMAP_ARGS = best_umap_hdbscan[0]\n",
    "BEST_HDBSCAN_ARGS = best_umap_hdbscan[1]\n",
    "BEST_MIN_COUNT = best_count_delta[0]\n",
    "BEST_MERGE_DELTA = best_count_delta[1]\n",
    "\n",
    "# Train final Top2Vec model\n",
    "top2vec_model = Top2Vec(\n",
    "    documents=documents,\n",
    "    embedding_model=BEST_EMBEDDING,\n",
    "    workers=12,\n",
    "    umap_args=BEST_UMAP_ARGS,\n",
    "    hdbscan_args=BEST_HDBSCAN_ARGS,\n",
    "    min_count=BEST_MIN_COUNT,\n",
    "    topic_merge_delta=BEST_MERGE_DELTA,\n",
    ")\n",
    "\n",
    "# Get topic words\n",
    "topics_words, _, _ = top2vec_model.get_topics()\n",
    "topic_words_list = [topic[:TOP_DIVERSITY_WORDS_N] for topic in topics_words]\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = models.CoherenceModel(\n",
    "    topics=topic_words_list,\n",
    "    texts=tokenized_texts,\n",
    "    dictionary=dictionary,\n",
    "    coherence='c_v',\n",
    "    topn=TOP_COHERENCE_WORDS_N,\n",
    ")\n",
    "coherence_score = coherence_model.get_coherence()\n",
    "print(f\"Final Top2Vec model coherence (c_v): {coherence_score:.4f}\")\n",
    "\n",
    "# Compute Topic Diversity Scores\n",
    "top_n_values = [5, 10, 20, 30]\n",
    "diversity_score_results = []\n",
    "for top_n in top_n_values:\n",
    "    diversity_score = topic_diversity(top2vec_model, top_n=top_n, model_type='top2vec')\n",
    "    diversity_score_results.append({\"top_n\": top_n, \"topic_diversity\": diversity_score})\n",
    "    print(f\"top_n: {top_n} topic_diversity: {diversity_score}\")\n",
    "\n",
    "# Save diversity scores\n",
    "df_diversity = pd.DataFrame(diversity_score_results)\n",
    "df_diversity.to_csv(os.path.join(results_dir, f\"top2vec_topic_diversity_scores_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save topic-word distributions\n",
    "topic_word_list = []\n",
    "for i, topic_words in enumerate(topics_words):\n",
    "    for word in topic_words[:TOP_DIVERSITY_WORDS_N]:\n",
    "        topic_word_list.append({\n",
    "            \"topic\": i,\n",
    "            \"word\": word\n",
    "        })\n",
    "df_topics = pd.DataFrame(topic_word_list)\n",
    "df_topics.to_csv(os.path.join(results_dir, f\"top2vec_topic_word_distributions_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save document-topic distribution\n",
    "doc_topics = []\n",
    "doc_topics_list = top2vec_model.get_documents_topics(doc_ids=list(range(len(documents))))\n",
    "\n",
    "for i, topic in enumerate(doc_topics_list):\n",
    "    row = {\n",
    "        \"doc_id\": i,\n",
    "        \"topic\": topic\n",
    "    }\n",
    "    doc_topics.append(row)\n",
    "\n",
    "df_doc_topics = pd.DataFrame(doc_topics)\n",
    "df_doc_topics.to_csv(os.path.join(results_dir, f\"top2vec_document_topic_distributions_{date_today}.csv\"), index=False)\n",
    "\n",
    "# Save model\n",
    "top2vec_model.save(os.path.join(results_dir, f\"top2vec_model_{date_today}.model\"))\n",
    "\n",
    "# Save Summary\n",
    "summary = {\n",
    "    \"embedding_model\": BEST_EMBEDDING,\n",
    "    \"min_count\": BEST_MIN_COUNT,\n",
    "    \"topic_merge_delta\": BEST_MERGE_DELTA,\n",
    "    \"umap_args\": BEST_UMAP_ARGS,\n",
    "    \"hdbscan_args\": BEST_HDBSCAN_ARGS,\n",
    "    \"coherence_score\": coherence_score,\n",
    "    \"dictionary_size\": len(dictionary),\n",
    "    \"num_documents\": len(documents),\n",
    "    \"num_topics\": len(topics_words),\n",
    "}\n",
    "for row in diversity_score_results:\n",
    "    summary[f\"diversity_score_top{row['top_n']}\"] = row[\"topic_diversity\"]\n",
    "\n",
    "pd.DataFrame([summary]).to_csv(os.path.join(results_dir, f\"top2vec_model_summary_{date_today}.csv\"), index=False)\n",
    "print(f\"Final Top2Vec model, topics, and summaries saved to: {results_dir}\")"
   ],
   "id": "db82c28ed6106cd4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### View Top-30 Words per Topic",
   "id": "78b3fe8088854c7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topics_words, _, _ = top2vec_model.get_topics()\n",
    "\n",
    "# Build structured list\n",
    "topic_word_data = []\n",
    "for topic_num, word_list in enumerate(topics_words):\n",
    "    for rank, word in enumerate(word_list, start=1):\n",
    "        topic_word_data.append({\n",
    "            \"topic\": topic_num,\n",
    "            \"word_rank\": rank,\n",
    "            \"word\": word,\n",
    "            \"weight\": None  # Top2Vec does not expose word weights directly\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_topic_words = pd.DataFrame(topic_word_data)\n",
    "\n",
    "# Save to CSV\n",
    "topic_words_filename = os.path.join(results_dir, f\"top2vec_top{TOP_DIVERSITY_WORDS_N}_words_per_topic_{date_today}.csv\")\n",
    "df_topic_words.to_csv(topic_words_filename, index=False)\n",
    "\n",
    "print(f\"Top {TOP_DIVERSITY_WORDS_N} words per topic saved to: {topic_words_filename}\")\n",
    "\n",
    "# Print Sample Preview\n",
    "print(f\"\\nTop {TOP_DIVERSITY_WORDS_N} Words per Topic:\")\n",
    "for topic_num in range(len(topics_words)):\n",
    "    words_only = topics_words[topic_num]\n",
    "    print(f\"Topic {topic_num}: {', '.join(words_only)}\")"
   ],
   "id": "e5b251b91c880ad5",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
