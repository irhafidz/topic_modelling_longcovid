üß† 5.2.1. Topic Interpretation Evaluation Methodology
We employed two complementary approaches to evaluate topic quality:
    Human Assessment
    Automated LLM Evaluation (using Deepseek R-1, Google Gemini 2.5 Pro, and ChatGPT o3)

üìù Evaluation Design

    Five topics per model were selected using coherence score stratification:
        Highest
        Upper Quartile (Q3)
        Median
        Lower Quartile (Q1)
        Lowest
    *Notes: This ensured a balanced representation of topic quality.
  
üß™ Human Evaluation Overview
The human evaluation was conducted via Google Forms from May 6th to May 20th, 2025, involving 15 English-proficient participants. Key demographic characteristics:
    Age group: 73.3% were aged 18‚Äì24 years
    Awareness: Over two-thirds had prior knowledge of Long COVID
    Relevance: This group represents a highly engaged demographic in health-related social media discourse

Each participant evaluated five representative topics per model using a 5-point Likert scale for:
    Interpretability: Clarity and coherence of topic themes
    Relevance: Connection to Long COVID discourse and experiences
The final scores were averaged per model for both criteria.


### üìä Interpretation Evaluation Results

![Bar Chart - Interpretation Evaluation](https://chart.googleapis.com/chart?cht=bvg&chs=700x400&chd=t:4.77,4.80,4.03,4.83,4.77|3.93,3.67,3.47,2.87,4.13|4.11,3.88,3.60,3.89,3.65|3.92,3.84,3.87,3.20,3.49&chxt=x,y&chxl=0:|LDA|NMF|LSA|Top2Vec|BERTopic&chxr=1,0,5&chds=0,5&chco=4F81BD,FFC000,9BBB59,C0504D&chdl=LLM+Interpretability|LLM+Relevance|Human+Interpretability|Human+Relevance&chtt=Interpretation+Evaluation+Scores)
